# 损失函数

loss = (W * X + b - Y)

y = wx + b + e(高斯噪声)

损失函数计算的是预测值和真实值之间的差值的平方和，使得损失差距最小。

## 损失函数计算

loss = ∑( w * x + b - y ) ^ 2

```python
# w：表示模型的权重（slope）。
# b：表示模型的偏置（intercept）。
# points：这是一个二维数组或矩阵，每一行代表一个样本点，其中第一列是输入特征值 x，第二列是对应的输出目标值 y。
def loss(w, b,points):
    totalError=0
    #使用一个 for 循环遍历输入数据集 points 中的所有样本
    for i in range(0,len(points)):
        x =  points[i,0]
        y =  points[i,1]
        #loss = ∑( w * x + b - y ) ^ 2
        totalError += (w*x + b - y) ** 2
    #循环结束后，将累计的误差平方和除以样本总数（用浮点数形式表示，防止整数除法），得到平均误差平方。
    return totalError / float(len(points))
```

## 梯度

> 损失函数对 w 和 b 的导数，分别表示模型权重 w 和偏置 b 的梯度。
> 在线性回归中，梯度（Gradient）是指损失函数相对于模型参数的偏导数向量。

### 梯度下降法

> 在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。


```python
# w：模型的权重（slope）初始值。
# b：模型的偏置（intercept）初始值。
# points：二维数组或矩阵，包含训练数据点，每一行是一个样本，其中第一列是特征值 x，第二列是目标值 y。
# learningRate：学习率，控制每次梯度更新的步长。
def gradient_descent(w, b, points, learningRate):
    b = 0
    w = 0
    N = float(len(points))
    # 遍历所有训练数据点，对于每个点计算其对损失函数关于 b 和 w 的梯度
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        # 分别对b和对w求导
        # loss = ∑( w * x + b - y ) ^ 2
        # 在循环体内，计算并立即更新了 b 和 w 的梯度，但这种方式会覆盖之前的数据点产生的梯度影响，导致最终得到的并不是梯度下降方向上的新解。
        b = -(2 / N) * (y - (w * x + b))
        w = -(2 / N) * (y - (w * x + b)) * x
    new_b = b - (learningRate * b)
    new_w = w - (learningRate * w)
    return [new_b, new_w]


def gradient_descent_run(points, b, w, learning_rate, num_iterations):
    for i in range(0, num_iterations):
        b, w = gradient_descent(w, b, points, learning_rate)
    return [b, w]
```